{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>ASSIGNMENT 2</center></h2>\n",
    "<h3><center>Mounica Subramani</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 [Linear regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "from sklearn import warnings \n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "KChouse_train = pd.read_csv('train.csv')\n",
    "KChouse_test = pd.read_csv('test.csv')\n",
    "\n",
    "# ignore the columns id, date, unnamed column as well as the categorical column zipcode.\n",
    "KChouse_train = KChouse_train.drop(columns = ['zipcode','Unnamed: 0'])\n",
    "KChouse_test = KChouse_test.drop(columns = ['id','zipcode','date','Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0  221900.0         3       1.00         1180      5650     1.0           0   \n",
       "1  538000.0         3       2.25         2570      7242     2.0           0   \n",
       "2  180000.0         2       1.00          770     10000     1.0           0   \n",
       "3  604000.0         4       3.00         1960      5000     1.0           0   \n",
       "4  510000.0         3       2.00         1680      8080     1.0           0   \n",
       "\n",
       "   view  condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "0     0          3      7        1180              0      1955             0   \n",
       "1     0          3      7        2170            400      1951          1991   \n",
       "2     0          3      6         770              0      1933             0   \n",
       "3     0          5      7        1050            910      1965             0   \n",
       "4     0          3      8        1680              0      1987             0   \n",
       "\n",
       "       lat     long  sqft_living15  sqft_lot15  \n",
       "0  47.5112 -122.257           1340        5650  \n",
       "1  47.7210 -122.319           1690        7639  \n",
       "2  47.7379 -122.233           2720        8062  \n",
       "3  47.5208 -122.393           1360        5000  \n",
       "4  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "KChouse_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price            1.000000\n",
       "sqft_living      0.704776\n",
       "grade            0.647349\n",
       "sqft_living15    0.645106\n",
       "sqft_above       0.582407\n",
       "bathrooms        0.487157\n",
       "view             0.445316\n",
       "sqft_basement    0.367365\n",
       "lat              0.365770\n",
       "waterfront       0.317143\n",
       "bedrooms         0.307058\n",
       "floors           0.239935\n",
       "sqft_lot15       0.161746\n",
       "sqft_lot         0.146645\n",
       "yr_renovated     0.146348\n",
       "condition        0.073961\n",
       "long             0.032846\n",
       "yr_built         0.016055\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation coefficients of features/variables\n",
    "KChouse_train.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Use an existing package to train a multiple linear regression model on the training set using all the\n",
    "features (except the ones excluded above). Report the coefficients of the linear regression models and\n",
    "the MSE metric on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.67837004 0.70416297 0.6125577  0.72692348 0.55953864 0.79355793\n",
      " 0.71355718 0.7597062  0.67484746 0.72691946]\n",
      "Average cross-validation score: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Linear regression model\n",
    "X_train = KChouse_train.drop(['price'], axis=1)\n",
    "y_train = KChouse_train[['price']]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n",
    "# print(\"X_train.shape:\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fitted.predict(X_train)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for train data: 31486167775.794888\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for train (unscaled) data:\",mean_squared_error(y_train,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Perform feature standardization so that each feature (including the response) has mean 0 and variance\n",
    "of 1. Train again a linear regression model on the training data and report the MSE on the training\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling data so that each feature has mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "data_scaled = scaler.fit_transform(KChouse_train)\n",
    "data_scaled_test = scaler.fit_transform(KChouse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87974769, -0.40982347, -1.44988843, ..., -0.35519332,\n",
       "        -0.96563661, -0.3128578 ],\n",
       "       [ 0.05182493, -0.40982347,  0.28318404, ..., -0.7998304 ,\n",
       "        -0.44332966, -0.23355563],\n",
       "       [-1.00323042, -1.58410276, -1.44988843, ..., -0.18307573,\n",
       "         1.09374507, -0.21669046],\n",
       "       ...,\n",
       "       [ 0.0975047 , -1.58410276, -1.44988843, ..., -0.86437449,\n",
       "        -1.02532883, -0.4185143 ],\n",
       "       [-0.97390696, -1.58410276, -1.44988843, ...,  1.07911986,\n",
       "        -0.80148299, -0.40352304],\n",
       "       [-0.68199849, -0.40982347, -0.06343045, ..., -0.46993837,\n",
       "         0.39236146, -0.15736334]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data_scaled[:,0:],\n",
    "                  index=data_scaled[:,0],\n",
    "                  columns=['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat',\n",
    "       'long', 'sqft_living15', 'sqft_lot15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 18)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.67837004 0.70416297 0.6125577  0.72692348 0.55953864 0.79355793\n",
      " 0.71355718 0.7597062  0.67484746 0.72691946]\n",
      "Average cross-validation score: 0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_scaled = df.drop(['price'], axis=1)\n",
    "y_train_scaled = df[['price']]\n",
    "\n",
    "model1 = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model1, X_train_scaled, y_train_scaled, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n",
    "# X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted1 = model1.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled = model_fitted1.predict(X_train_scaled)\n",
    "# predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03690325,  0.05460245,  0.16724347,  0.03206976,  0.0237055 ,\n",
       "         0.18785555,  0.14204967,  0.03820676,  0.27181372,  0.14231485,\n",
       "         0.07997506, -0.19934981,  0.05090017,  0.23097972, -0.00305083,\n",
       "         0.13432109, -0.03810604]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fitted1.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for scaled train data: 0.2734665681293983\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for scaled train data:\",mean_squared_error(y_train_scaled,predictions_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Evaluate both models on the testing set. Report the MSE on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression model on unscaled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.572243   0.7302379  0.4195207  0.69421544 0.58056117 0.64406129\n",
      " 0.58310599 0.67480098 0.69470867 0.5749148 ]\n",
      "Average cross-validation score: 0.62\n"
     ]
    }
   ],
   "source": [
    "X_test = KChouse_test.drop(['price'], axis=1)\n",
    "y_test = KChouse_test[['price']]\n",
    "\n",
    "model2 = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model2, X_test, y_test, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted2 = model2.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model_fitted2.predict(X_test)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.09706680e+04,  4.53522145e+04,  1.29557364e+02,\n",
       "         4.60107448e-01, -4.08290867e+03,  6.67916885e+05,\n",
       "         6.41825495e+04,  3.74395531e+04,  8.69134817e+04,\n",
       "         7.58952045e+01,  5.36621579e+01, -2.59297980e+03,\n",
       "        -5.13466279e+00,  5.67370348e+05, -7.67745545e+04,\n",
       "         3.41095820e+01, -8.48109171e-01]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fitted2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for non scaled test data: 54185036855.79511\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for non scaled test data:\",mean_squared_error(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression on scaled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(data=data_scaled_test[1:,0:],\n",
    "                  index=data_scaled_test[1:,0],\n",
    "                  columns=['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat',\n",
    "       'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "df_scaled = df_scaled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.61750399 0.72177335 0.41520807 0.69590594 0.58111104 0.63899562\n",
      " 0.58430498 0.67685041 0.6919166  0.57445912]\n",
      "Average cross-validation score: 0.62\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = df_scaled.drop(['price'], axis=1)\n",
    "y_test_scaled = df_scaled[['price']]\n",
    "\n",
    "model3 = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model3, X_test_scaled, y_test_scaled, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted3 = model3.fit(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled1 = model_fitted3.predict(X_test_scaled)\n",
    "# predictions_scaled1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11656073,  0.08800974,  0.23817802,  0.0661224 , -0.00525078,\n",
       "         0.1852539 ,  0.12709521,  0.06128675,  0.25616884,  0.21405969,\n",
       "         0.08813423, -0.17705287, -0.00531226,  0.19009029, -0.02545619,\n",
       "         0.05815344, -0.0584464 ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fitted3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared errorfor scaled test data: 0.3253062062957616\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for scaled test data:\",mean_squared_error(y_test_scaled,predictions_scaled1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Interpret the results in your own words. Which features contribute mostly to the linear regression\n",
    "model? Is the model fitting the data well? How large is the model error?\n",
    "\n",
    "- Grade feature contributes mostly to the linear regression model. It has the highest coefficient value of 0.25616884.\n",
    "- We din generalize the model well by taking all the features of the data into modelling.\n",
    "- The model is fitting the data well and it is evident from the r-sqaured value calculated below as the difference between r squared calculated for training and testing has no much difference. \n",
    "- The error is not large and it is appropriate for the data used and the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared error on testing data: 0.6749701848743102\n",
      "Rsquared error on training data: 0.7265334318706018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_test = r2_score(y_test_scaled, predictions_scaled1)\n",
    "print(\"Rsquared error on testing data:\",r2_test)\n",
    "\n",
    "r2_train = r2_score(y_train_scaled,predictions_scaled)\n",
    "print(\"Rsquared error on training data:\",r2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 [Closed-form solution for linear regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Implement simple linear regression using the closed form and train a model for one feature (sqft living)\n",
    "using the training set. Write code to predict a response for a new single-dimensional data point in the\n",
    "testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple linear regression with one feature(sqft living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\mouni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# obtain the feature matrix \n",
    "KChouse_train1 = df.drop(columns = ['price'])\n",
    "KChouse_test1 = df_scaled.drop(columns = ['price'])\n",
    "\n",
    "X = KChouse_train1[['sqft_living15']]\n",
    "X['ones'] = 1\n",
    "X_test = KChouse_test1[['sqft_living15']]\n",
    "X_test['ones'] = 1\n",
    "\n",
    "# obtain the target variable \n",
    "y = df[['price']]\n",
    "y_test = df_scaled[['price']]\n",
    "\n",
    "# calculate coefficients using closed-form solution\n",
    "coeffs_CF = inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "\n",
    "y_prediction = X.dot(coeffs_CF)\n",
    "y_test_prediction = X_test.dot(coeffs_CF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>coeffs_CF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sqft_living15</td>\n",
       "      <td>6.451060e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ones</td>\n",
       "      <td>6.288373e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Features     coeffs_CF\n",
       "0  sqft_living15  6.451060e-01\n",
       "1           ones  6.288373e-17"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the feature names into list\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "feature_names\n",
    "\n",
    "# convert list to array\n",
    "\n",
    "features = np.asarray(feature_names)\n",
    "features\n",
    "\n",
    "# convert both the array vectors into dataframe\n",
    "res = pd.DataFrame(coeffs_CF)\n",
    "\n",
    "res1 = pd.DataFrame(features)\n",
    "\n",
    "# merge dataframes\n",
    "results = pd.merge(res1, res, left_index=True, right_index=True)\n",
    "results = results.rename(columns={'0_x':'Features','0_y':'coeffs_CF'})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.285995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.705582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.603684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.180098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.622938\n",
       "1 -0.285995\n",
       "2  0.705582\n",
       "3 -0.603684\n",
       "4 -0.180098"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.355080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.057839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.292845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.596589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.355080\n",
       "1 -0.141438\n",
       "2 -0.057839\n",
       "3 -0.292845\n",
       "4 -0.596589"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data: 0.5838382382385958\n",
      "Mean squared error for testing data: 0.6764263056279269\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for training data (simple Linear regression):\",mean_squared_error(y,y_prediction))\n",
    "print(\"Mean squared error for testing data (simple Linear regression):\",mean_squared_error(y_test,y_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implement the closed-from solution for multiple linear regression using matrix operations and train a\n",
    "model on the training set. Write code to predict a response for a new multi-dimensional data point in\n",
    "the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = KChouse_train1\n",
    "X1['ones'] = 1\n",
    "\n",
    "X_test1 = KChouse_test1\n",
    "X_test1['ones'] = 1\n",
    "\n",
    "# obtain the target variable \n",
    "y1 = df[['price']]\n",
    "y_test1 = df_scaled[['price']]\n",
    "\n",
    "# calculate coefficients using closed-form solution\n",
    "coeffs_CF_1 = inv(X1.transpose().dot(X1)).dot(X1.transpose()).dot(y1)\n",
    "\n",
    "y_prediction1 = X1.dot(coeffs_CF_1)\n",
    "y_test_prediction1 = X_test1.dot(coeffs_CF_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>coeffs_CF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.819630e-02</td>\n",
       "      <td>bedrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.082002e-01</td>\n",
       "      <td>bathrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.465047e-01</td>\n",
       "      <td>sqft_living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.959751e-02</td>\n",
       "      <td>sqft_lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.108606e-02</td>\n",
       "      <td>floors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.888729e-01</td>\n",
       "      <td>waterfront</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.634773e-01</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.165114e-02</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.387074e-01</td>\n",
       "      <td>grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.340260e-01</td>\n",
       "      <td>sqft_above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.157826e-01</td>\n",
       "      <td>sqft_basement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.993498e-01</td>\n",
       "      <td>yr_built</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.090017e-02</td>\n",
       "      <td>yr_renovated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.309797e-01</td>\n",
       "      <td>lat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3.050828e-03</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.343211e-01</td>\n",
       "      <td>sqft_living15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.810604e-02</td>\n",
       "      <td>sqft_lot15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.165437e-15</td>\n",
       "      <td>ones</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Features      coeffs_CF\n",
       "0   1.819630e-02       bedrooms\n",
       "1   2.082002e-01      bathrooms\n",
       "2  -5.465047e-01    sqft_living\n",
       "3   1.959751e-02       sqft_lot\n",
       "4   1.108606e-02         floors\n",
       "5   1.888729e-01     waterfront\n",
       "6   1.634773e-01           view\n",
       "7   4.165114e-02      condition\n",
       "8   2.387074e-01          grade\n",
       "9   8.340260e-01     sqft_above\n",
       "10  4.157826e-01  sqft_basement\n",
       "11 -1.993498e-01       yr_built\n",
       "12  5.090017e-02   yr_renovated\n",
       "13  2.309797e-01            lat\n",
       "14 -3.050828e-03           long\n",
       "15  1.343211e-01  sqft_living15\n",
       "16 -3.810604e-02     sqft_lot15\n",
       "17  3.165437e-15           ones"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the feature names into list\n",
    "\n",
    "feature_names1 = list(X1.columns)\n",
    "# print(feature_names1)\n",
    "\n",
    "# convert list to array\n",
    "\n",
    "features1 = np.asarray(feature_names1)\n",
    "# print(features1)\n",
    "\n",
    "# convert both the array vectors into dataframe\n",
    "res2 = pd.DataFrame(coeffs_CF_1)\n",
    "\n",
    "res3 = pd.DataFrame(features1)\n",
    "\n",
    "# merge dataframes\n",
    "results1 = pd.merge(res2, res3, left_index=True, right_index=True)\n",
    "results1 = results1.rename(columns={'0_x':'Features','0_y':'coeffs_CF'})\n",
    "\n",
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.979190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.658167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.586131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.232246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.979190\n",
       "1  0.658167\n",
       "2 -0.586131\n",
       "3 -0.085220\n",
       "4 -0.232246"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.766049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.971737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.221909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.026984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.766049\n",
       "1 -0.971737\n",
       "2  0.011118\n",
       "3 -1.221909\n",
       "4 -0.026984"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_prediction1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Compare the models given by your implementation with those trained in Problem 1 by the R or Python\n",
    "packages. Report the MSE metrics for the models you implemented on both training and testing sets\n",
    "\n",
    "- The package generated linear regression model has appropriate mean squared error value just right for the data used.\n",
    "- The implemented model's mean squared error values a bit high than package model, but the difference is quite neglegible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE for Python package used models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for scaled train data: 0.2734665681293983\n",
      "Mean squared errorfor scaled test data: 0.3253062062957616\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for scaled train data:\",mean_squared_error(y_train_scaled,predictions_scaled))\n",
    "print(\"Mean squared errorfor scaled test data:\",mean_squared_error(y_test_scaled,predictions_scaled1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE for implemented model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data: 0.31194738550839385\n",
      "Mean squared error for testing data: 0.37329326533579754\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for training data:\",mean_squared_error(y1,y_prediction1))\n",
    "print(\"Mean squared error for testing data:\",mean_squared_error(y_test1,y_test_prediction1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 [Gradient descent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = KChouse_train1\n",
    "X2['ones'] = 1\n",
    "\n",
    "X_test2 = KChouse_test1\n",
    "X_test2['ones'] = 1\n",
    "\n",
    "# obtain the target variable \n",
    "y2 = df[['price']]\n",
    "y_test2 = df_scaled[['price']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write code for gradient descent for training linear regression using the algorithm from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent1(X,y,theta,alpha,n):\n",
    "    \n",
    "    m = len(y)\n",
    "        \n",
    "    for i in range(n):\n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        theta = theta - (1/m) * alpha * (X.T.dot(prediction - y))\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Vary the value of the learning rate (3 different values) and report the value of θ after different number\n",
    "of iterations (10, 50, and 100). Include the MSE metric on the training and testing set for all values of\n",
    "θ and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error on scaled test data: 17.464163451221626\n",
      "mean squared error on scaled test data: 6.97958920199715\n",
      "mean squared error on scaled test data: 3.445182931877422\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(2)\n",
    "alpha = 0.01\n",
    "iter = [10,50,100]\n",
    "cols = KChouse_train1.columns\n",
    "theta = np.random.randn(len(cols),1)\n",
    "theta_df = theta\n",
    "for i in iter:\n",
    "    theta_j = gradient_descent1(X2,y2,theta_df,alpha,i)\n",
    "    y_test_pred_gd = X_test2.dot(theta_j)\n",
    "    print(\"mean squared error on scaled test data (GD):\",mean_squared_error(y_test2,y_test_pred_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error on scaled test data: 1.413135824269324\n",
      "mean squared error on scaled test data: 0.361498220452008\n",
      "mean squared error on scaled test data: 0.34150739669043473\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "iter = [10,50,100]\n",
    "cols = KChouse_train1.columns\n",
    "theta = np.random.randn(len(cols),1)\n",
    "theta_df = theta\n",
    "for i in iter:\n",
    "    theta_j = gradient_descent1(X2,y2,theta_df,alpha,i)\n",
    "    y_test_pred_gd = X_test2.dot(theta_j)\n",
    "    print(\"mean squared error on scaled test data (GD):\",mean_squared_error(y_test2,y_test_pred_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error on scaled test data: 1.5245356316084024\n",
      "mean squared error on scaled test data: 0.3824370082195767\n",
      "mean squared error on scaled test data: 0.3449699649933864\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "iter = [10,50,100]\n",
    "cols = KChouse_train1.columns\n",
    "theta = np.random.randn(len(cols),1)\n",
    "theta_df = theta\n",
    "for i in iter:\n",
    "    theta_j = gradient_descent1(X2,y2,theta_df,alpha,i)\n",
    "    y_test_pred_gd = X_test2.dot(theta_j)\n",
    "    print(\"mean squared error on scaled test data (GD):\",mean_squared_error(y_test2,y_test_pred_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error on scaled test data: 1.8160006493541812\n",
      "mean squared error on scaled test data: 0.40963304455213345\n",
      "mean squared error on scaled test data: 0.3573510297650286\n",
      "mean squared error on scaled test data: 0.34246820956709867\n",
      "mean squared error on scaled test data: 0.3409119071387248\n",
      "mean squared error on scaled test data: 0.3406447292674675\n",
      "mean squared error on scaled test data: 0.3405909841654776\n",
      "mean squared error on scaled test data: 0.34057982865194064\n",
      "mean squared error on scaled test data: 0.34057750551290006\n",
      "mean squared error on scaled test data: 0.34057702248141386\n",
      "mean squared error on scaled test data: 0.34057692224133757\n",
      "mean squared error on scaled test data: 0.3405769014708617\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "iter = [10,50,100,200,300,400,500,600,700,800,900,1000]\n",
    "cols = KChouse_train1.columns\n",
    "theta = np.random.randn(len(cols),1)\n",
    "theta_df = theta\n",
    "for i in iter:\n",
    "    theta_j = gradient_descent1(X2,y2,theta_df,alpha,i)\n",
    "    y_test_pred_gd = X_test2.dot(theta_j)\n",
    "    print(\"mean squared error on scaled test data (GD):\",mean_squared_error(y_test2,y_test_pred_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write the derivation of the closed form solution for parameter θ that minimizes the loss function J(θ)\n",
    "in ridge regression.\n",
    "\n",
    "- In paper format at the end of the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Modify your linear regression implementation from Problem 2 to handle ridge regression. Take several values of the regularization parameter λ and output the MSE metric. Plot the value of MSE as a\n",
    "function of λ. What is the best value of λ that you found? Compare the results of linear regression\n",
    "and ridge regression on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data (ridge): 0.31194738550839385\n",
      "Mean squared error for testing data (ridge): 0.37329326533579754\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27357928858440955\n",
      "Mean squared error for testing data (ridge): 0.3402268155372297\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27387872101234956\n",
      "Mean squared error for testing data (ridge): 0.34027999083813976\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2743231393764978\n",
      "Mean squared error for testing data (ridge): 0.3404495886233099\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2748848144849361\n",
      "Mean squared error for testing data (ridge): 0.34071532960019146\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27554425161640084\n",
      "Mean squared error for testing data (ridge): 0.3410631869726186\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2762870722453272\n",
      "Mean squared error for testing data (ridge): 0.34148287414599043\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.277102234053876\n",
      "Mean squared error for testing data (ridge): 0.341966462191841\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27798096659100985\n",
      "Mean squared error for testing data (ridge): 0.3425075830816734\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27891610801660377\n",
      "Mean squared error for testing data (ridge): 0.34310095097301613\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.27990167604354055\n",
      "Mean squared error for testing data (ridge): 0.3437420632398855\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2809325807156815\n",
      "Mean squared error for testing data (ridge): 0.3444270067699917\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.28200442596952\n",
      "Mean squared error for testing data (ridge): 0.3451523279478502\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2831133684788264\n",
      "Mean squared error for testing data (ridge): 0.3459149423530667\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2842560145083424\n",
      "Mean squared error for testing data (ridge): 0.3467120699491553\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.28542934265521624\n",
      "Mean squared error for testing data (ridge): 0.34754118709422116\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2866306446591221\n",
      "Mean squared error for testing data (ridge): 0.34839998995780264\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.28785747911648085\n",
      "Mean squared error for testing data (ridge): 0.34928636587973166\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2891076346109302\n",
      "Mean squared error for testing data (ridge): 0.35019837040432145\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2903790998547345\n",
      "Mean squared error for testing data (ridge): 0.3511342084735976\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2916700391492076\n",
      "Mean squared error for testing data (ridge): 0.35209221874302743\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.29297877195148697\n",
      "Mean squared error for testing data (ridge): 0.3530708602958413\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.29430375566291617\n",
      "Mean squared error for testing data (ridge): 0.3540687012395934\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2956435709825937\n",
      "Mean squared error for testing data (ridge): 0.35508440880894215\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.29699690933122275\n",
      "Mean squared error for testing data (ridge): 0.3561167406952055\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2983625619665918\n",
      "Mean squared error for testing data (ridge): 0.35716453739092224\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.2997394104968199\n",
      "Mean squared error for testing data (ridge): 0.35822671538589906\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3011264185603035\n",
      "Mean squared error for testing data (ridge): 0.3593022610862243\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.30252262448844575\n",
      "Mean squared error for testing data (ridge): 0.3603902253535811\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.30392713480310213\n",
      "Mean squared error for testing data (ridge): 0.36148971858157997\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.30533911842828126\n",
      "Mean squared error for testing data (ridge): 0.36259990624063665\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3067578015171429\n",
      "Mean squared error for testing data (ridge): 0.3637200048343915\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3081824628122753\n",
      "Mean squared error for testing data (ridge): 0.3648492782196938\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.30961242947070716\n",
      "Mean squared error for testing data (ridge): 0.36598703424937484\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3110470732959466\n",
      "Mean squared error for testing data (ridge): 0.36713262170285915\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.31248580732811687\n",
      "Mean squared error for testing data (ridge): 0.3682854274744279\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.31392808275044726\n",
      "Mean squared error for testing data (ridge): 0.3694448739928876\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.31537338607629656\n",
      "Mean squared error for testing data (ridge): 0.37061041684969775\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.31682123658580325\n",
      "Mean squared error for testing data (ridge): 0.3717815426153839\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3182711839853701\n",
      "Mean squared error for testing data (ridge): 0.37295776682642806\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3197228062666523\n",
      "Mean squared error for testing data (ridge): 0.3741386321268488\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.32117570774464177\n",
      "Mean squared error for testing data (ridge): 0.3753237065504282\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.32262951725693917\n",
      "Mean squared error for testing data (ridge): 0.3765125819310488\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.32408388650843273\n",
      "Mean squared error for testing data (ridge): 0.37770487242992823\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3255384885474401\n",
      "Mean squared error for testing data (ridge): 0.3789002131696804\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3269930163609463\n",
      "Mean squared error for testing data (ridge): 0.380098258966154\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.32844718157794717\n",
      "Mean squared error for testing data (ridge): 0.3812986831498834\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.32990071327109766\n",
      "Mean squared error for testing data (ridge): 0.3825011764697844\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.33135335684790984\n",
      "Mean squared error for testing data (ridge): 0.3837054460724232\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.33280487302365264\n",
      "Mean squared error for testing data (ridge): 0.3849112145508136\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3342550368689135\n",
      "Mean squared error for testing data (ridge): 0.38611821905725147\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3357036369254865\n",
      "Mean squared error for testing data (ridge): 0.38732621047519983\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.33715047438487444\n",
      "Mean squared error for testing data (ridge): 0.38853495264567445\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3385953623242524\n",
      "Mean squared error for testing data (ridge): 0.38974422164399114\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3400381249952225\n",
      "Mean squared error for testing data (ridge): 0.39095380510308714\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.34147859716113804\n",
      "Mean squared error for testing data (ridge): 0.39216350157996116\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3429166234791569\n",
      "Mean squared error for testing data (ridge): 0.39337311996206414\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.34435205792353923\n",
      "Mean squared error for testing data (ridge): 0.3945824789107431\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.34578476324701435\n",
      "Mean squared error for testing data (ridge): 0.395791406339075\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3472146104773235\n",
      "Mean squared error for testing data (ridge): 0.3969997389216509\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.34864147844629656\n",
      "Mean squared error for testing data (ridge): 0.3982073216340617\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3500652533490513\n",
      "Mean squared error for testing data (ridge): 0.3994140073200243\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.35148582833110287\n",
      "Mean squared error for testing data (ridge): 0.4006196562842403\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.35290310310136336\n",
      "Mean squared error for testing data (ridge): 0.4018241359092386\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3543169835691722\n",
      "Mean squared error for testing data (ridge): 0.40302732029458005\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.35572738150365785\n",
      "Mean squared error for testing data (ridge): 0.40422908991693385\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.35713421421385816\n",
      "Mean squared error for testing data (ridge): 0.4054293313096412\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3585374042481641\n",
      "Mean squared error for testing data (ridge): 0.40662793676049136\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.35993687911175537\n",
      "Mean squared error for testing data (ridge): 0.40782480402652815\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.36133257100080635\n",
      "Mean squared error for testing data (ridge): 0.4090198360647893\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3627244165523323\n",
      "Mean squared error for testing data (ridge): 0.4102129407779651\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.36411235660863245\n",
      "Mean squared error for testing data (ridge): 0.4114040307740346\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data (ridge): 0.3654963359953668\n",
      "Mean squared error for testing data (ridge): 0.41259302313900115\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3668763033123738\n",
      "Mean squared error for testing data (ridge): 0.41377983922191847\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.36825221073640374\n",
      "Mean squared error for testing data (ridge): 0.4149644044314494\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3696240138350016\n",
      "Mean squared error for testing data (ridge): 0.4161466480432525\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.37099167139083034\n",
      "Mean squared error for testing data (ridge): 0.4173265030175461\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3723551452357748\n",
      "Mean squared error for testing data (ridge): 0.41850390582623337\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3737144000942155\n",
      "Mean squared error for testing data (ridge): 0.41967879628902605\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3750694034349027\n",
      "Mean squared error for testing data (ridge): 0.42085111741803216\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.37642012533090213\n",
      "Mean squared error for testing data (ridge): 0.42202081527031304\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3777665383271196\n",
      "Mean squared error for testing data (ridge): 0.4231878388079483\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.37910861731494616\n",
      "Mean squared error for testing data (ridge): 0.424352139765175\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3804463394135967\n",
      "Mean squared error for testing data (ridge): 0.42551367252219857\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.38177968385774286\n",
      "Mean squared error for testing data (ridge): 0.42667239398529394\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.383108631891068\n",
      "Mean squared error for testing data (ridge): 0.427828263472847\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.38443316666539884\n",
      "Mean squared error for testing data (ridge): 0.4289812426070019\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3857532731450862\n",
      "Mean squared error for testing data (ridge): 0.43013129521060356\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.38706893801633374\n",
      "Mean squared error for testing data (ridge): 0.431278387209145\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.38838014960119205\n",
      "Mean squared error for testing data (ridge): 0.4324224865374459\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3896868977759479\n",
      "Mean squared error for testing data (ridge): 0.4335635630508049\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.390989173893666\n",
      "Mean squared error for testing data (ridge): 0.4347015884403856\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39228697071064605\n",
      "Mean squared error for testing data (ridge): 0.4358365361526098\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39358028231657805\n",
      "Mean squared error for testing data (ridge): 0.4369683813123436\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39486910406819187\n",
      "Mean squared error for testing data (ridge): 0.4380971006496799\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39615343252620777\n",
      "Mean squared error for testing data (ridge): 0.4392226724301242\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39743326539540885\n",
      "Mean squared error for testing data (ridge): 0.4403450763880112\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.3987086014676641\n",
      "Mean squared error for testing data (ridge): 0.4414642936629812\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.39997944056774487\n",
      "Mean squared error for testing data (ridge): 0.4425803067393616\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.40124578350178386\n",
      "Mean squared error for testing data (ridge): 0.44369309938830587\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4025076320082378\n",
      "Mean squared error for testing data (ridge): 0.44480265661254803\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.40376498871121663\n",
      "Mean squared error for testing data (ridge): 0.445908964593641\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4050178570760623\n",
      "Mean squared error for testing data (ridge): 0.447012010641558\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.40626624136705175\n",
      "Mean squared error for testing data (ridge): 0.4481117831465345\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.40751014660711915\n",
      "Mean squared error for testing data (ridge): 0.44920827153304355\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4087495785394909\n",
      "Mean squared error for testing data (ridge): 0.450301466215799\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.40998454359113484\n",
      "Mean squared error for testing data (ridge): 0.4513913585576854\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4112150488379322\n",
      "Mean squared error for testing data (ridge): 0.4524779408295252\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.41244110197148465\n",
      "Mean squared error for testing data (ridge): 0.4535612061715897\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.41366271126747206\n",
      "Mean squared error for testing data (ridge): 0.4546411485567745\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.41487988555548594\n",
      "Mean squared error for testing data (ridge): 0.45571776275535686\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.41609263419026277\n",
      "Mean squared error for testing data (ridge): 0.456791044301262\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4173009670242483\n",
      "Mean squared error for testing data (ridge): 0.45786098945976594\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4185048943814275\n",
      "Mean squared error for testing data (ridge): 0.45892759519656856\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4197044270323569\n",
      "Mean squared error for testing data (ridge): 0.4599908591481719\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4208995761703411\n",
      "Mean squared error for testing data (ridge): 0.46105077959350327\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4220903533886992\n",
      "Mean squared error for testing data (ridge): 0.4621073554267281\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4232767706590644\n",
      "Mean squared error for testing data (ridge): 0.4631605861311934\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4244588403106717\n",
      "Mean squared error for testing data (ridge): 0.46421047175445534\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.42563657501058305\n",
      "Mean squared error for testing data (ridge): 0.4652570128843375\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.426809987744807\n",
      "Mean squared error for testing data (ridge): 0.46630021062597604\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4279790918002689\n",
      "Mean squared error for testing data (ridge): 0.46734006657980637\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4291439007475927\n",
      "Mean squared error for testing data (ridge): 0.4683765828204489\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4303044284246553\n",
      "Mean squared error for testing data (ridge): 0.4694097618764551\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4314606889208783\n",
      "Mean squared error for testing data (ridge): 0.47043960671087404\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4326126965622211\n",
      "Mean squared error for testing data (ridge): 0.4714661207026056\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.433760465896844\n",
      "Mean squared error for testing data (ridge): 0.47248930762850333\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.43490401168141124\n",
      "Mean squared error for testing data (ridge): 0.47350917164619666\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.43604334886800133\n",
      "Mean squared error for testing data (ridge): 0.4745257172775997\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.43717849259160013\n",
      "Mean squared error for testing data (ridge): 0.47553894939307695\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4383094581581486\n",
      "Mean squared error for testing data (ridge): 0.47654887319623934\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.43943626103311995\n",
      "Mean squared error for testing data (ridge): 0.4775554942093418\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44055891683060255\n",
      "Mean squared error for testing data (ridge): 0.47855881825925684\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44167744130286535\n",
      "Mean squared error for testing data (ridge): 0.4795588514640021\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44279185033038493\n",
      "Mean squared error for testing data (ridge): 0.4805556002197948\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44390215991231263\n",
      "Mean squared error for testing data (ridge): 0.4815490711886148\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44500838615736255\n",
      "Mean squared error for testing data (ridge): 0.48253927128625096\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.44611054527510174\n",
      "Mean squared error for testing data (ridge): 0.48352620767081483\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4472086535676248\n",
      "Mean squared error for testing data (ridge): 0.48450988773169945\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4483027274215954\n",
      "Mean squared error for testing data (ridge): 0.4854903190789653\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data (ridge): 0.44939278330063986\n",
      "Mean squared error for testing data (ridge): 0.4864675095331373\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4504788377380756\n",
      "Mean squared error for testing data (ridge): 0.4874414671153946\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4515609073299609\n",
      "Mean squared error for testing data (ridge): 0.48841220003813685\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.45263900872845286\n",
      "Mean squared error for testing data (ridge): 0.48937971669591385\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4537131586354573\n",
      "Mean squared error for testing data (ridge): 0.4903440256567011\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.45478337379656214\n",
      "Mean squared error for testing data (ridge): 0.4913051356535101\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4558496709952377\n",
      "Mean squared error for testing data (ridge): 0.49226305557631667\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4569120670472951\n",
      "Mean squared error for testing data (ridge): 0.49321779446429803\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.457970578795592\n",
      "Mean squared error for testing data (ridge): 0.4941693614983651\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4590252231049729\n",
      "Mean squared error for testing data (ridge): 0.49511776599397544\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4600760168574361\n",
      "Mean squared error for testing data (ridge): 0.496063017394222\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4611229769475166\n",
      "Mean squared error for testing data (ridge): 0.4970051252631801\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4621661202778759\n",
      "Mean squared error for testing data (ridge): 0.4979440992795071\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4632054637550905\n",
      "Mean squared error for testing data (ridge): 0.49887994923028317\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.46424102428563035\n",
      "Mean squared error for testing data (ridge): 0.4998126850050841\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.46527281877201887\n",
      "Mean squared error for testing data (ridge): 0.5007423165902761\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.46630086410916716\n",
      "Mean squared error for testing data (ridge): 0.501668854063526\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4673251771808763\n",
      "Mean squared error for testing data (ridge): 0.5025923075885175\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.468345774856499\n",
      "Mean squared error for testing data (ridge): 0.5035126874098653\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4693626739877539\n",
      "Mean squared error for testing data (ridge): 0.5044300038482191\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.47037589140568853\n",
      "Mean squared error for testing data (ridge): 0.5053442672955522\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4713854439177819\n",
      "Mean squared error for testing data (ridge): 0.5062554882106258\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4723913483051827\n",
      "Mean squared error for testing data (ridge): 0.5071636771146234\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4733936213200768\n",
      "Mean squared error for testing data (ridge): 0.5080688445869475\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4743922796831792\n",
      "Mean squared error for testing data (ridge): 0.508971001261175\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.47538734008134487\n",
      "Mean squared error for testing data (ridge): 0.5098701578211624\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.47637881916529357\n",
      "Mean squared error for testing data (ridge): 0.5107663249972978\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4773667335474443\n",
      "Mean squared error for testing data (ridge): 0.5116595135628917\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.47835109979985474\n",
      "Mean squared error for testing data (ridge): 0.5125497343307043\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.47933193445226185\n",
      "Mean squared error for testing data (ridge): 0.5134369981496021\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.480309253990218\n",
      "Mean squared error for testing data (ridge): 0.514321315901339\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4812830748533204\n",
      "Mean squared error for testing data (ridge): 0.5152026984974593\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4822534134335298\n",
      "Mean squared error for testing data (ridge): 0.5160811568763155\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4832202860735734\n",
      "Mean squared error for testing data (ridge): 0.516956702000199\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.48418370906543046\n",
      "Mean squared error for testing data (ridge): 0.5178293448525787\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.48514369864889634\n",
      "Mean squared error for testing data (ridge): 0.5186990964354418\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.48610027101022035\n",
      "Mean squared error for testing data (ridge): 0.5195659677667366\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4870534422808184\n",
      "Mean squared error for testing data (ridge): 0.5204299698779113\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.48800322853605294\n",
      "Mean squared error for testing data (ridge): 0.5212911138115454\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4889496457940795\n",
      "Mean squared error for testing data (ridge): 0.5221494106190699\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4898927100147583\n",
      "Mean squared error for testing data (ridge): 0.5230048713585765\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4908324370986258\n",
      "Mean squared error for testing data (ridge): 0.5238575070927073\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.49176884288592587\n",
      "Mean squared error for testing data (ridge): 0.5247073288866263\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4927019431556968\n",
      "Mean squared error for testing data (ridge): 0.525554347806068\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.49363175362491224\n",
      "Mean squared error for testing data (ridge): 0.5263985749154613\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.49455828994767514\n",
      "Mean squared error for testing data (ridge): 0.5272400212761242\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.49548156771445967\n",
      "Mean squared error for testing data (ridge): 0.5280786979445291\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.49640160245140297\n",
      "Mean squared error for testing data (ridge): 0.528914615970634\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4973184096196413\n",
      "Mean squared error for testing data (ridge): 0.5297477863962806\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4982320046146906\n",
      "Mean squared error for testing data (ridge): 0.5305782202536515\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.4991424027658693\n",
      "Mean squared error for testing data (ridge): 0.5314059285637903\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5000496193357623\n",
      "Mean squared error for testing data (ridge): 0.5322309223351785\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5009536695197223\n",
      "Mean squared error for testing data (ridge): 0.5330532125623691\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5018545684454099\n",
      "Mean squared error for testing data (ridge): 0.5338728102246733\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5027523311723683\n",
      "Mean squared error for testing data (ridge): 0.5346897262848999\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5036469726916338\n",
      "Mean squared error for testing data (ridge): 0.5355039716881463\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5045385079253774\n",
      "Mean squared error for testing data (ridge): 0.5363155573606356\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5054269517265793\n",
      "Mean squared error for testing data (ridge): 0.5371244942086024\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5063123188787334\n",
      "Mean squared error for testing data (ridge): 0.537930793117224\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5071946240955804\n",
      "Mean squared error for testing data (ridge): 0.5387344649495935\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5080738820208691\n",
      "Mean squared error for testing data (ridge): 0.5395355205457384\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5089501072281446\n",
      "Mean squared error for testing data (ridge): 0.5403339707216755\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5098233142205617\n",
      "Mean squared error for testing data (ridge): 0.5411298262685093\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data (ridge): 0.510693517430723\n",
      "Mean squared error for testing data (ridge): 0.5419230979515659\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5115607312205411\n",
      "Mean squared error for testing data (ridge): 0.5427137965095649\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5124249698811213\n",
      "Mean squared error for testing data (ridge): 0.5435019326538252\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.51328624763267\n",
      "Mean squared error for testing data (ridge): 0.544287517067507\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5141445786244179\n",
      "Mean squared error for testing data (ridge): 0.5450705604048847\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5149999769345697\n",
      "Mean squared error for testing data (ridge): 0.5458510732906541\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5158524565702673\n",
      "Mean squared error for testing data (ridge): 0.5466290663192668\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5167020314675743\n",
      "Mean squared error for testing data (ridge): 0.5474045500542994\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5175487154914766\n",
      "Mean squared error for testing data (ridge): 0.5481775350278467\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5183925224359007\n",
      "Mean squared error for testing data (ridge): 0.5489480317399444\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5192334660237464\n",
      "Mean squared error for testing data (ridge): 0.5497160506580201\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5200715599069374\n",
      "Mean squared error for testing data (ridge): 0.5504816022163679\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5209068176664827\n",
      "Mean squared error for testing data (ridge): 0.5512446968156488\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5217392528125566\n",
      "Mean squared error for testing data (ridge): 0.5520053448224158\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5225688787845879\n",
      "Mean squared error for testing data (ridge): 0.5527635565686617\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5233957089513644\n",
      "Mean squared error for testing data (ridge): 0.5535193423513894\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5242197566111488\n",
      "Mean squared error for testing data (ridge): 0.5542727124322044\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5250410349918065\n",
      "Mean squared error for testing data (ridge): 0.555023677036928\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5258595572509442\n",
      "Mean squared error for testing data (ridge): 0.5557722463552301\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5266753364760597\n",
      "Mean squared error for testing data (ridge): 0.5565184305402837\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5274883856847015\n",
      "Mean squared error for testing data (ridge): 0.557262239708436\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5282987178246379\n",
      "Mean squared error for testing data (ridge): 0.5580036839388992\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5291063457740358\n",
      "Mean squared error for testing data (ridge): 0.5587427732734576\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5299112823416487\n",
      "Mean squared error for testing data (ridge): 0.559479517716194\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5307135402670117\n",
      "Mean squared error for testing data (ridge): 0.5602139272332303\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.531513132220646\n",
      "Mean squared error for testing data (ridge): 0.5609460117524854\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5323100708042698\n",
      "Mean squared error for testing data (ridge): 0.5616757811634479\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5331043685510178\n",
      "Mean squared error for testing data (ridge): 0.5624032453169634\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5338960379256669\n",
      "Mean squared error for testing data (ridge): 0.5631284140250371\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.534685091324867\n",
      "Mean squared error for testing data (ridge): 0.563851297060649\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5354715410773814\n",
      "Mean squared error for testing data (ridge): 0.5645719041575833\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5362553994443299\n",
      "Mean squared error for testing data (ridge): 0.5652902450102713\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5370366786194395\n",
      "Mean squared error for testing data (ridge): 0.5660063292736444\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5378153907292985\n",
      "Mean squared error for testing data (ridge): 0.5667201665630017\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5385915478336185\n",
      "Mean squared error for testing data (ridge): 0.567431766453888\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5393651619254981\n",
      "Mean squared error for testing data (ridge): 0.5681411384819832\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.540136244931693\n",
      "Mean squared error for testing data (ridge): 0.5688482921430023\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5409048087128896\n",
      "Mean squared error for testing data (ridge): 0.5695532368926062\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5416708650639831\n",
      "Mean squared error for testing data (ridge): 0.5702559821463223\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5424344257143588\n",
      "Mean squared error for testing data (ridge): 0.5709565372794755\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5431955023281778\n",
      "Mean squared error for testing data (ridge): 0.5716549116271272\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5439541065046639\n",
      "Mean squared error for testing data (ridge): 0.5723511144840249\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5447102497783979\n",
      "Mean squared error for testing data (ridge): 0.5730451551045589\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5454639436196086\n",
      "Mean squared error for testing data (ridge): 0.5737370427027301\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.546215199434473\n",
      "Mean squared error for testing data (ridge): 0.5744267864521229\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5469640285654149\n",
      "Mean squared error for testing data (ridge): 0.5751143954858869\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5477104422914068\n",
      "Mean squared error for testing data (ridge): 0.5757998788967278\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5484544518282741\n",
      "Mean squared error for testing data (ridge): 0.5764832457369032\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.549196068329003\n",
      "Mean squared error for testing data (ridge): 0.5771645050182279\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5499353028840468\n",
      "Mean squared error for testing data (ridge): 0.5778436657120826\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5506721665216369\n",
      "Mean squared error for testing data (ridge): 0.578520736749433\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5514066702080939\n",
      "Mean squared error for testing data (ridge): 0.579195727020853\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5521388248481417\n",
      "Mean squared error for testing data (ridge): 0.5798686453765537\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5528686412852202\n",
      "Mean squared error for testing data (ridge): 0.5805395006264191\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5535961303018018\n",
      "Mean squared error for testing data (ridge): 0.5812083015400478\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5543213026197077\n",
      "Mean squared error for testing data (ridge): 0.5818750568467989\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5550441689004251\n",
      "Mean squared error for testing data (ridge): 0.5825397752358439\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5557647397454261\n",
      "Mean squared error for testing data (ridge): 0.5832024653562238\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5564830256964858\n",
      "Mean squared error for testing data (ridge): 0.5838631358169106\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5571990372360027\n",
      "Mean squared error for testing data (ridge): 0.5845217951868737\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5579127847873183\n",
      "Mean squared error for testing data (ridge): 0.5851784519951501\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.558624278715038\n",
      "Mean squared error for testing data (ridge): 0.5858331147309205\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5593335293253509\n",
      "Mean squared error for testing data (ridge): 0.5864857918435875\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5600405468663519\n",
      "Mean squared error for testing data (ridge): 0.5871364917428594\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5607453415283628\n",
      "Mean squared error for testing data (ridge): 0.5877852227988378\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5614479234442524\n",
      "Mean squared error for testing data (ridge): 0.5884319933421068\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5621483026897588\n",
      "Mean squared error for testing data (ridge): 0.5890768116638285\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5628464892838099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for testing data (ridge): 0.5897196860158405\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5635424931888443\n",
      "Mean squared error for testing data (ridge): 0.5903606246107564\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5642363243111322\n",
      "Mean squared error for testing data (ridge): 0.5909996356220701\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5649279925010948\n",
      "Mean squared error for testing data (ridge): 0.5916367271842626\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5656175075536257\n",
      "Mean squared error for testing data (ridge): 0.5922719073929122\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5663048792084091\n",
      "Mean squared error for testing data (ridge): 0.5929051843048077\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5669901171502391\n",
      "Mean squared error for testing data (ridge): 0.5935365659380615\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5676732310093384\n",
      "Mean squared error for testing data (ridge): 0.5941660602722302\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5683542303616758\n",
      "Mean squared error for testing data (ridge): 0.5947936752484327\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5690331247292834\n",
      "Mean squared error for testing data (ridge): 0.5954194187694742\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5697099235805726\n",
      "Mean squared error for testing data (ridge): 0.5960432986999701\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5703846363306512\n",
      "Mean squared error for testing data (ridge): 0.5966653228664727\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5710572723416361\n",
      "Mean squared error for testing data (ridge): 0.5972854990576003\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5717278409229687\n",
      "Mean squared error for testing data (ridge): 0.5979038350241687\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5723963513317281\n",
      "Mean squared error for testing data (ridge): 0.598520338479323\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5730628127729424\n",
      "Mean squared error for testing data (ridge): 0.5991350170986711\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5737272343999005\n",
      "Mean squared error for testing data (ridge): 0.599747878520422\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5743896253144617\n",
      "Mean squared error for testing data (ridge): 0.6003589303455216\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5750499945673659\n",
      "Mean squared error for testing data (ridge): 0.6009681801377917\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.575708351158541\n",
      "Mean squared error for testing data (ridge): 0.6015756354240717\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5763647040374097\n",
      "Mean squared error for testing data (ridge): 0.6021813036943595\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5770190621031951\n",
      "Mean squared error for testing data (ridge): 0.6027851924019553\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5776714342052272\n",
      "Mean squared error for testing data (ridge): 0.6033873089636047\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5783218291432437\n",
      "Mean squared error for testing data (ridge): 0.603987660759646\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5789702556676946\n",
      "Mean squared error for testing data (ridge): 0.6045862551341551\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.579616722480042\n",
      "Mean squared error for testing data (ridge): 0.6051830993950937\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5802612382330605\n",
      "Mean squared error for testing data (ridge): 0.6057782008144583\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5809038115311355\n",
      "Mean squared error for testing data (ridge): 0.606371566628429\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5815444509305597\n",
      "Mean squared error for testing data (ridge): 0.6069632040375192\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5821831649398305\n",
      "Mean squared error for testing data (ridge): 0.6075531202067284\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5828199620199439\n",
      "Mean squared error for testing data (ridge): 0.6081413222656917\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5834548505846869\n",
      "Mean squared error for testing data (ridge): 0.6087278173088343\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5840878390009309\n",
      "Mean squared error for testing data (ridge): 0.6093126123955233\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5847189355889207\n",
      "Mean squared error for testing data (ridge): 0.6098957145502215\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.585348148622565\n",
      "Mean squared error for testing data (ridge): 0.6104771307626419\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5859754863297225\n",
      "Mean squared error for testing data (ridge): 0.6110568679879018\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5866009568924894\n",
      "Mean squared error for testing data (ridge): 0.6116349331466793\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5872245684474839\n",
      "Mean squared error for testing data (ridge): 0.6122113331253664\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5878463290861293\n",
      "Mean squared error for testing data (ridge): 0.612786074776227\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5884662468549356\n",
      "Mean squared error for testing data (ridge): 0.6133591649175524\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5890843297557814\n",
      "Mean squared error for testing data (ridge): 0.6139306103338174\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.589700585746191\n",
      "Mean squared error for testing data (ridge): 0.6145004177758373\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5903150227396132\n",
      "Mean squared error for testing data (ridge): 0.6150685939609244\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.590927648605697\n",
      "Mean squared error for testing data (ridge): 0.6156351455730463\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5915384711705657\n",
      "Mean squared error for testing data (ridge): 0.6162000792629818\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5921474982170906\n",
      "Mean squared error for testing data (ridge): 0.6167634016484784\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5927547374851616\n",
      "Mean squared error for testing data (ridge): 0.6173251193144111\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5933601966719586\n",
      "Mean squared error for testing data (ridge): 0.6178852388129376\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5939638834322187\n",
      "Mean squared error for testing data (ridge): 0.6184437666636579\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5945658053785039\n",
      "Mean squared error for testing data (ridge): 0.6190007093537707\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5951659700814662\n",
      "Mean squared error for testing data (ridge): 0.6195560733382306\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5957643850701115\n",
      "Mean squared error for testing data (ridge): 0.6201098650399067\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5963610578320627\n",
      "Mean squared error for testing data (ridge): 0.6206620908497386\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.59695599581382\n",
      "Mean squared error for testing data (ridge): 0.6212127571268944\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.59754920642102\n",
      "Mean squared error for testing data (ridge): 0.6217618701989271\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5981406970186947\n",
      "Mean squared error for testing data (ridge): 0.6223094363619325\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5987304749315265\n",
      "Mean squared error for testing data (ridge): 0.6228554618807048\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5993185474441038\n",
      "Mean squared error for testing data (ridge): 0.6233999529888938\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.5999049218011739\n",
      "Mean squared error for testing data (ridge): 0.6239429158891608\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6004896052078952\n",
      "Mean squared error for testing data (ridge): 0.6244843567533354\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6010726048300872\n",
      "Mean squared error for testing data (ridge): 0.62502428172257\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6016539277944785\n",
      "Mean squared error for testing data (ridge): 0.6255626969074967\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6022335811889554\n",
      "Mean squared error for testing data (ridge): 0.6260996083883817\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6028115720628066\n",
      "Mean squared error for testing data (ridge): 0.6266350222152804\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6033879074269675\n",
      "Mean squared error for testing data (ridge): 0.6271689444081923\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6039625942542622\n",
      "Mean squared error for testing data (ridge): 0.6277013809572142\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.604535639479647\n",
      "Mean squared error for testing data (ridge): 0.6282323378226958\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6051070500004466\n",
      "Mean squared error for testing data (ridge): 0.6287618209353916\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6056768326765948\n",
      "Mean squared error for testing data (ridge): 0.6292898361966147\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6062449943308704\n",
      "Mean squared error for testing data (ridge): 0.6298163894783904\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6068115417491318\n",
      "Mean squared error for testing data (ridge): 0.630341486623606\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6073764816805514\n",
      "Mean squared error for testing data (ridge): 0.6308651334461657\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6079398208378468\n",
      "Mean squared error for testing data (ridge): 0.6313873357311401\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.608501565897513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for testing data (ridge): 0.6319080992349179\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.60906172350005\n",
      "Mean squared error for testing data (ridge): 0.6324274296853558\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6096203002501911\n",
      "Mean squared error for testing data (ridge): 0.6329453327819297\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6101773027171292\n",
      "Mean squared error for testing data (ridge): 0.6334618141958835\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.610732737434742\n",
      "Mean squared error for testing data (ridge): 0.633976879570378\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6112866109018148\n",
      "Mean squared error for testing data (ridge): 0.6344905345206404\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6118389295822619\n",
      "Mean squared error for testing data (ridge): 0.6350027846341112\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6123896999053482\n",
      "Mean squared error for testing data (ridge): 0.6355136354705926\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.612938928265907\n",
      "Mean squared error for testing data (ridge): 0.6360230925623959\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6134866210245591\n",
      "Mean squared error for testing data (ridge): 0.6365311614144868\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6140327845079269\n",
      "Mean squared error for testing data (ridge): 0.6370378475046325\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6145774250088509\n",
      "Mean squared error for testing data (ridge): 0.6375431562835462\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6151205487866018\n",
      "Mean squared error for testing data (ridge): 0.6380470931750332\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6156621620670929\n",
      "Mean squared error for testing data (ridge): 0.6385496635761336\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6162022710430906\n",
      "Mean squared error for testing data (ridge): 0.6390508728572678\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6167408818744233\n",
      "Mean squared error for testing data (ridge): 0.6395507263623783\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6172780006881892\n",
      "Mean squared error for testing data (ridge): 0.640049229409073\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6178136335789625\n",
      "Mean squared error for testing data (ridge): 0.6405463872887673\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.618347786608998\n",
      "Mean squared error for testing data (ridge): 0.6410422052668246\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6188804658084359\n",
      "Mean squared error for testing data (ridge): 0.6415366885826987\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6194116771755022\n",
      "Mean squared error for testing data (ridge): 0.6420298424500726\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6199414266767108\n",
      "Mean squared error for testing data (ridge): 0.6425216720569991\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6204697202470629\n",
      "Mean squared error for testing data (ridge): 0.6430121825660391\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6209965637902446\n",
      "Mean squared error for testing data (ridge): 0.6435013791144008\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6215219631788236\n",
      "Mean squared error for testing data (ridge): 0.6439892668140772\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.622045924254446\n",
      "Mean squared error for testing data (ridge): 0.644475850751983\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6225684528280288\n",
      "Mean squared error for testing data (ridge): 0.6449611359900916\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6230895546799536\n",
      "Mean squared error for testing data (ridge): 0.645445127565571\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6236092355602582\n",
      "Mean squared error for testing data (ridge): 0.6459278304909193\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6241275011888268\n",
      "Mean squared error for testing data (ridge): 0.6464092497540986\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6246443572555783\n",
      "Mean squared error for testing data (ridge): 0.6468893903186708\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.625159809420655\n",
      "Mean squared error for testing data (ridge): 0.6473682571239289\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6256738633146081\n",
      "Mean squared error for testing data (ridge): 0.6478458550850319\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6261865245385837\n",
      "Mean squared error for testing data (ridge): 0.6483221890931354\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6266977986645056\n",
      "Mean squared error for testing data (ridge): 0.6487972640155242\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6272076912352592\n",
      "Mean squared error for testing data (ridge): 0.6492710846957431\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6277162077648717\n",
      "Mean squared error for testing data (ridge): 0.649743655953727\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6282233537386923\n",
      "Mean squared error for testing data (ridge): 0.6502149825859299\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6287291346135726\n",
      "Mean squared error for testing data (ridge): 0.6506850693654558\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.629233555818042\n",
      "Mean squared error for testing data (ridge): 0.6511539210421848\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.629736622752486\n",
      "Mean squared error for testing data (ridge): 0.6516215423429029\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6302383407893206\n",
      "Mean squared error for testing data (ridge): 0.6520879379714273\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6307387152731666\n",
      "Mean squared error for testing data (ridge): 0.6525531126087347\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.631237751521022\n",
      "Mean squared error for testing data (ridge): 0.6530170709130859\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6317354548224343\n",
      "Mean squared error for testing data (ridge): 0.6534798175201509\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6322318304396709\n",
      "Mean squared error for testing data (ridge): 0.6539413570431344\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6327268836078879\n",
      "Mean squared error for testing data (ridge): 0.6544016940728988\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6332206195352985\n",
      "Mean squared error for testing data (ridge): 0.6548608331780881\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6337130434033406\n",
      "Mean squared error for testing data (ridge): 0.6553187789052501\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6342041603668415\n",
      "Mean squared error for testing data (ridge): 0.6557755357789592\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6346939755541837\n",
      "Mean squared error for testing data (ridge): 0.6562311083019363\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.635182494067468\n",
      "Mean squared error for testing data (ridge): 0.6566855009551706\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6356697209826757\n",
      "Mean squared error for testing data (ridge): 0.6571387181980399\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6361556613498298\n",
      "Mean squared error for testing data (ridge): 0.6575907644684288\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.636640320193157\n",
      "Mean squared error for testing data (ridge): 0.6580416441828494\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.637123702511244\n",
      "Mean squared error for testing data (ridge): 0.6584913617365574\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6376058132771976\n",
      "Mean squared error for testing data (ridge): 0.658939921503671\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6380866574388006\n",
      "Mean squared error for testing data (ridge): 0.6593873278372875\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6385662399186683\n",
      "Mean squared error for testing data (ridge): 0.6598335850695991\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6390445656144031\n",
      "Mean squared error for testing data (ridge): 0.6602786975120096\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6395216393987477\n",
      "Mean squared error for testing data (ridge): 0.6607226694552484\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6399974661197384\n",
      "Mean squared error for testing data (ridge): 0.6611655051694852\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6404720506008564\n",
      "Mean squared error for testing data (ridge): 0.6616072089044442\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.640945397641178\n",
      "Mean squared error for testing data (ridge): 0.6620477848895164\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data (ridge): 0.6414175120155248\n",
      "Mean squared error for testing data (ridge): 0.6624872373338729\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6418883984746119\n",
      "Mean squared error for testing data (ridge): 0.6629255704265765\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.642358061745195\n",
      "Mean squared error for testing data (ridge): 0.6633627883366927\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6428265065302173\n",
      "Mean squared error for testing data (ridge): 0.6637988952134012\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6432937375089548\n",
      "Mean squared error for testing data (ridge): 0.6642338951861045\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6437597593371607\n",
      "Mean squared error for testing data (ridge): 0.6646677923645393\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.644224576647208\n",
      "Mean squared error for testing data (ridge): 0.6651005908388834\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.644688194048234\n",
      "Mean squared error for testing data (ridge): 0.6655322946798651\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6451506161262793\n",
      "Mean squared error for testing data (ridge): 0.6659629079388706\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6456118474444303\n",
      "Mean squared error for testing data (ridge): 0.6663924346480505\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6460718925429574\n",
      "Mean squared error for testing data (ridge): 0.6668208788204268\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6465307559394539\n",
      "Mean squared error for testing data (ridge): 0.6672482444499986\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6469884421289738\n",
      "Mean squared error for testing data (ridge): 0.6676745355118465\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6474449555841684\n",
      "Mean squared error for testing data (ridge): 0.6680997559622386\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6479003007554219\n",
      "Mean squared error for testing data (ridge): 0.6685239097387333\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.648354482070986\n",
      "Mean squared error for testing data (ridge): 0.6689470007602831\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6488075039371141\n",
      "Mean squared error for testing data (ridge): 0.6693690329273382\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6492593707381937\n",
      "Mean squared error for testing data (ridge): 0.6697900101219466\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6497100868368794\n",
      "Mean squared error for testing data (ridge): 0.6702099362078585\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6501596565742223\n",
      "Mean squared error for testing data (ridge): 0.6706288150306254\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.650608084269802\n",
      "Mean squared error for testing data (ridge): 0.6710466504177011\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6510553742218553\n",
      "Mean squared error for testing data (ridge): 0.6714634461785418\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6515015307074031\n",
      "Mean squared error for testing data (ridge): 0.6718792061047053\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6519465579823807\n",
      "Mean squared error for testing data (ridge): 0.6722939339699499\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6523904602817621\n",
      "Mean squared error for testing data (ridge): 0.6727076335303325\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6528332418196868\n",
      "Mean squared error for testing data (ridge): 0.6731203085243054\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6532749067895847\n",
      "Mean squared error for testing data (ridge): 0.6735319626728149\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6537154593642996\n",
      "Mean squared error for testing data (ridge): 0.673942599679397\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6541549036962133\n",
      "Mean squared error for testing data (ridge): 0.6743522232302722\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6545932439173671\n",
      "Mean squared error for testing data (ridge): 0.6747608369944434\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6550304841395838\n",
      "Mean squared error for testing data (ridge): 0.6751684446237882\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6554666284545881\n",
      "Mean squared error for testing data (ridge): 0.6755750497531551\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6559016809341269\n",
      "Mean squared error for testing data (ridge): 0.6759806560004551\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6563356456300878\n",
      "Mean squared error for testing data (ridge): 0.6763852669667566\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6567685265746175\n",
      "Mean squared error for testing data (ridge): 0.6767888862363781\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6572003277802394\n",
      "Mean squared error for testing data (ridge): 0.6771915173769785\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.65763105323997\n",
      "Mean squared error for testing data (ridge): 0.6775931639396507\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6580607069274348\n",
      "Mean squared error for testing data (ridge): 0.677993829459011\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6584892927969834\n",
      "Mean squared error for testing data (ridge): 0.6783935174532902\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6589168147838036\n",
      "Mean squared error for testing data (ridge): 0.6787922314244237\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6593432768040347\n",
      "Mean squared error for testing data (ridge): 0.6791899748581405\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6597686827548808\n",
      "Mean squared error for testing data (ridge): 0.6795867512240521\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6601930365147215\n",
      "Mean squared error for testing data (ridge): 0.6799825639757414\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6606163419432248\n",
      "Mean squared error for testing data (ridge): 0.6803774165508497\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6610386028814558\n",
      "Mean squared error for testing data (ridge): 0.6807713123711641\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6614598231519874\n",
      "Mean squared error for testing data (ridge): 0.6811642548427053\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6618800065590085\n",
      "Mean squared error for testing data (ridge): 0.6815562473558119\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6622991568884329\n",
      "Mean squared error for testing data (ridge): 0.6819472932852286\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6627172779080063\n",
      "Mean squared error for testing data (ridge): 0.6823373959901896\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6631343733674123\n",
      "Mean squared error for testing data (ridge): 0.6827265588145034\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6635504469983803\n",
      "Mean squared error for testing data (ridge): 0.6831147850866385\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6639655025147881\n",
      "Mean squared error for testing data (ridge): 0.6835020781198056\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6643795436127689\n",
      "Mean squared error for testing data (ridge): 0.6838884412120413\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6647925739708137\n",
      "Mean squared error for testing data (ridge): 0.6842738776462912\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6652045972498742\n",
      "Mean squared error for testing data (ridge): 0.684658390690492\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6656156170934668\n",
      "Mean squared error for testing data (ridge): 0.6850419835976528\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6660256371277729\n",
      "Mean squared error for testing data (ridge): 0.6854246596059369\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6664346609617396\n",
      "Mean squared error for testing data (ridge): 0.6858064219387421\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6668426921871822\n",
      "Mean squared error for testing data (ridge): 0.6861872738047823\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6672497343788807\n",
      "Mean squared error for testing data (ridge): 0.6865672183981648\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6676557910946825\n",
      "Mean squared error for testing data (ridge): 0.6869462588984725\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6680608658755972\n",
      "Mean squared error for testing data (ridge): 0.6873243984708405\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6684649622458969\n",
      "Mean squared error for testing data (ridge): 0.687701640266036\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6688680837132115\n",
      "Mean squared error for testing data (ridge): 0.6880779874205355\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6692702337686263\n",
      "Mean squared error for testing data (ridge): 0.6884534430566026\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6696714158867765\n",
      "Mean squared error for testing data (ridge): 0.6888280102823645\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6700716335259437\n",
      "Mean squared error for testing data (ridge): 0.6892016921918894\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.670470890128149\n",
      "Mean squared error for testing data (ridge): 0.6895744918652615\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6708691891192474\n",
      "Mean squared error for testing data (ridge): 0.689946412368658\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6712665339090208\n",
      "Mean squared error for testing data (ridge): 0.6903174567544222\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6716629278912715\n",
      "Mean squared error for testing data (ridge): 0.6906876280611409\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.672058374443912\n",
      "Mean squared error for testing data (ridge): 0.691056929313717\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6724528769290583\n",
      "Mean squared error for testing data (ridge): 0.6914253635234427\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6728464386931198\n",
      "Mean squared error for testing data (ridge): 0.6917929336880753\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6732390630668893\n",
      "Mean squared error for testing data (ridge): 0.6921596427919072\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6736307533656328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for testing data (ridge): 0.6925254938058414\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.674021512889178\n",
      "Mean squared error for testing data (ridge): 0.6928904896874606\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6744113449220028\n",
      "Mean squared error for testing data (ridge): 0.6932546333811017\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6748002527333233\n",
      "Mean squared error for testing data (ridge): 0.6936179278179246\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6751882395771804\n",
      "Mean squared error for testing data (ridge): 0.6939803759159847\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6755753086925265\n",
      "Mean squared error for testing data (ridge): 0.6943419805803028\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6759614633033115\n",
      "Mean squared error for testing data (ridge): 0.6947027447029339\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6763467066185689\n",
      "Mean squared error for testing data (ridge): 0.695062671163039\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6767310418324987\n",
      "Mean squared error for testing data (ridge): 0.6954217628269516\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.677114472124554\n",
      "Mean squared error for testing data (ridge): 0.6957800225482491\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6774970006595229\n",
      "Mean squared error for testing data (ridge): 0.6961374531678187\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6778786305876131\n",
      "Mean squared error for testing data (ridge): 0.6964940575139268\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6782593650445327\n",
      "Mean squared error for testing data (ridge): 0.6968498384022853\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6786392071515743\n",
      "Mean squared error for testing data (ridge): 0.6972047986361205\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6790181600156947\n",
      "Mean squared error for testing data (ridge): 0.6975589410062373\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6793962267295972\n",
      "Mean squared error for testing data (ridge): 0.6979122682910877\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6797734103718113\n",
      "Mean squared error for testing data (ridge): 0.6982647832568355\n",
      "\n",
      "\n",
      "Mean squared error for training data (ridge): 0.6801497140067727\n",
      "Mean squared error for testing data (ridge): 0.6986164886574214\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I = np.identity(18)\n",
    "lamda = []\n",
    "MSE = []\n",
    "\n",
    "# derivation of the closed form solution for parameter θ that minimizes the loss function J(θ) in ridge regression\n",
    "\n",
    "for lamda_i in range(0,10000,20):\n",
    "    lamda.append(lamda_i)\n",
    "    coeffs_CF_2 = inv(X1.transpose().dot(X1) + lamda_i * I).dot(X1.transpose()).dot(y1)   \n",
    "    # Y prediction values for training data\n",
    "    y_prediction1_rg = X1.dot(coeffs_CF_2)\n",
    "    # y predictiom values for testing data\n",
    "    y_test_prediction1_rg = X_test1.dot(coeffs_CF_2)\n",
    "    \n",
    "    mean_error = mean_squared_error(y1,y_prediction1_rg)\n",
    "    mean_error_test = mean_squared_error(y_test1,y_test_prediction1_rg)\n",
    "    \n",
    "    print(\"Mean squared error for training data (ridge):\",mean_squared_error(y1,y_prediction1_rg))    \n",
    "    print(\"Mean squared error for testing data (ridge):\",mean_squared_error(y_test1,y_test_prediction1_rg))\n",
    "    print(\"\\n\")\n",
    "    MSE.append(mean_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lamda</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.373293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.340227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>0.340280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.340450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>0.340715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lamda       MSE\n",
       "0      0  0.373293\n",
       "1     20  0.340227\n",
       "2     40  0.340280\n",
       "3     60  0.340450\n",
       "4     80  0.340715"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list to array\n",
    "\n",
    "Lamda = np.asarray(lamda)\n",
    "MSE_df = np.asarray(MSE)\n",
    "# print(features1)\n",
    "\n",
    "# convert both the array vectors into dataframe\n",
    "res_1 = pd.DataFrame(Lamda)\n",
    "\n",
    "res_2 = pd.DataFrame(MSE_df)\n",
    "\n",
    "# merge dataframes\n",
    "result_df = pd.merge(res_1, res_2, left_index=True, right_index=True)\n",
    "result_df = result_df.rename(columns={'0_x':'Lamda','0_y':'MSE'})\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x291fc58b978>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df.plot.line(x='MSE', y='Lamda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for scaled train data: 0.2734665681293983\n",
      "Mean squared errorfor scaled test data: 0.3253062062957616\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error for scaled train data:\",mean_squared_error(y_train_scaled,predictions_scaled))\n",
    "print(\"Mean squared errorfor scaled test data:\",mean_squared_error(y_test_scaled,predictions_scaled1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best value for lambda is 1, below are the MSE values for lamba = 1.\n",
    "\n",
    "######  Ridge regression MSE\n",
    "- Mean squared error for training data (ridge): 0.27357928858440955\n",
    "- Mean squared error for testing data (ridge): 0.3402268155372297\n",
    "\n",
    "###### package based linear regression MSE\n",
    "- Mean squared error for scaled train data: 0.2734665681293983\n",
    "- Mean squared errorfor scaled test data: 0.3253062062957616\n",
    "\n",
    "There is no much difference in MSE values of rigde and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 [Dependent features in linear regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In paper format at the end of the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 [Laplace noise in MLE estimate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Plot the Laplace pdf for b ∈ {1, 2, 4}. How is the Laplace distribution different from the normal\n",
    "distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x291fdad32e8>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loc, scale = 0., 1.\n",
    "x_lap = np.random.laplace(loc, scale, 1000)\n",
    "\n",
    "x = np.arange(-8., 8., .01)\n",
    "pdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\n",
    "plt.plot(x, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x291fdb31588>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loc, scale = 0., 2.\n",
    "x_lap = np.random.laplace(loc, scale, 1000)\n",
    "\n",
    "x = np.arange(-8., 8., .01)\n",
    "pdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\n",
    "plt.plot(x, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x291fdb908d0>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loc, scale = 0., 4.\n",
    "x_lap = np.random.laplace(loc, scale, 1000)\n",
    "\n",
    "x = np.arange(-8., 8., .01)\n",
    "pdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\n",
    "plt.plot(x, pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Derive the objective J(θ) that maximizes the MLE for Laplace noise. The objective should depend\n",
    "on θ, the training dataset (xi, yi), for i ∈ {1, . . . , N}, and b.\n",
    "\n",
    "- In paper format at the end of the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Consider an outlier in the training data, defined as a point of high residual. Which of the two objectives\n",
    "(derived for normal or Laplace noise) are more resilient to the effect of outliers?\n",
    "\n",
    "- The pdf of laplace distribution is (1/2b)*e^(-x/b) and the MLE is 1.\n",
    "- The pdf of normal distribution is (1/sqrt(2pi))*e^(x^2/b) and the MLE is 2x.\n",
    "- The normal distribution is more vulnerable to outliers.\n",
    "- Therefore, laplace noise is more resilient to the effect of outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
